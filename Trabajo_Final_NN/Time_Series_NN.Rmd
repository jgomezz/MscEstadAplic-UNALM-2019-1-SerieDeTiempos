---
title: "Time Series and Neural Networks"
author: "E. Figueroa, J. Gómez, R. León"
date: "27 de junio de 2019"
output: html_document
---

## Preparando el entorno y la información para el análisis.

Cargamos los paquetes para el análisis.

```{r message=FALSE}
# Paquetes para el análisis

library(TSA)
library(ggfortify)
library(Kendall)
library(randtests)
library(pracma)
library(forecast)
library(trend)
library(DescTools)
library(lubridate)

```

Utilizaremos para nuestro ejemplo a analizar Apple, el titan tecnológico que en el 2019 es la empresa con mayor capitalización bursátil del mundo. 

Revisaremos las cotizaciones diarias de Apple, cuyo ticker en el NYSE es AAPL, utilizaremos el precio ajustado que es el que incorpora los cambios corporativos como entrega de dividendos, splits, etc.

El periodo a analizar es de enero 2000 a junio 2019.

```{r}
df_stock <- read.csv("aapl_2000_01_to_2019_06.csv", header = T, stringsAsFactors = F)

df_stock$date <- as.Date(df_stock$date)

str(df_stock)

head(df_stock)

frecuencia_st <- round(mean(table(year ( df_stock$date[year(df_stock$date) <= 2018] ))))


st <- ts( df_stock['adjusted'], 
          freq = frecuencia_st, 
          start = c(2000,1,3) )

stock_name <- "AAPL"

```

### Análisis Exploratorio de datos

#### Revisión de Tendencia

Aplicaremos los siguientes medios:

* Inspección gráfica de la serie
* Revisión gráfica de la autocorrelación
* Prueba de hipótesis de Cox-Stuart
* Prueba Rho de Spearman


##### Inspección Gráfica

```{r}

autoplot(st) + 
  labs(title = paste("Cotizaciones diarias", stock_name),
       x     = "tiempo",
       y     = "Cotización en USD") + 
  theme_minimal()



```

**Conclusión**

Se aprecia que existe tendencias por tramos, la primera descendente de 2015 - 2016, luego ascendente desde mediados del 2016 hasta mediados del 2018 y descendente hasta finales de ese año. Finalmente, ascendente hasta abril de 2019 y luego descendente.

En una perspectiva general se puede decir que la tendencia es aproximadamente creciente.



##### Revisión de autocorrelación

```{r}

autoplot(TSA::acf(st, type = "correlation", lag = 4900, plot = FALSE)) +
    labs(title = paste("Autocorrelación diaria para la acción", stock_name),
       x     = "lag",
       y     = "correlación")

```


**Conclusión**
Se aprecia que existe fuerte autocorrelación pero por tramos tal como se había sospechado en el análisis gráfico.

##### Prueba de Cox-Stuart


```{r}
alpha_test <- 0.05

test_cox_diferente <- cox.stuart.test(st)

test_cox_creciente <- cox.stuart.test(st, alternative = c("right.sided"))

test_cox_decreciente <- cox.stuart.test(st, alternative = c("left.sided"))

if (test_cox_diferente$p.value < alpha_test) {
  print( paste('Se rechaza H0', 'existe evidencia de tendencia')) } else{
  print( 'No se rechaza H0, no hay evidencia de tendencia')
}

if (test_cox_creciente$p.value < alpha_test) {
  print( paste('Se rechaza H0', 'existe evidencia de tendencia creciente')) } else{
  print( 'No se rechaza H0, no hay evidencia de tendencia creciente')
  }

if (test_cox_decreciente$p.value < alpha_test) {
  print( paste('Se rechaza H0', 'existe evidencia de tendencia decreciente')) } else{
  print( 'No se rechaza H0, no hay evidencia de tendencia decreciente')
  }

```

**Conclusión**

Existe evidencias para suponer tendencia creciente

##### Prueba Rho de Spearman

```{r message=FALSE}

test_spearman <- cor.test(st, time(st), method = "spearman")

if (test_spearman$p.value < alpha_test) {
  print( paste('Se rechaza H0', 'por lo que existe evidencia de autocorrelación'))
  print( paste('La correlación es ', test_spearman$estimate,
               'por lo que existe evidencia de',
               ifelse(test_spearman$estimate < 0, 
                      'tendencia decreciente',
                      'tendencia creciente') ))
  } else{
  print( 'No se rechaza H0, no hay evidencia de autocorrelación')
  }


```

#### Revisión de Estacionalidad

##### Correlograma

```{r}

autoplot(TSA::acf(st, type = "correlation", lag = 1200, plot = FALSE)) +
    labs(title = paste("Autocorrelación diaria para la acción", stock_name),
       x     = "lag",
       y     = "correlación")

```


** Conclusión **
No se evidencia la presencia de estacionalidad desde el punto de vista gráfico.

##### Gráfico de Cajas

```{r}

seasonplot(st, s=12 )

monthplot(st)

boxplot(df_stock$adjusted ~ month(df_stock$date), col = "lightblue")


```

**Conclusión**
Se observa que no existe estacionalidad, y la dispersión es grande.

##### Prueba de kruskal Wallis

```{r}

test_kruskal <- kruskal.test(st ~ cycle(st))

if (test_kruskal$p.value < alpha_test) {
  print( paste('Se rechaza H0', 'existe evidencia de estacionalidad')) } else{
  print( 'No se rechaza H0, no hay evidencia de estacionalidad')
  }


```
### Revisión de modelo de Regresión Lineal

Modelo: Y ~ alpha + Beta1 * t + mes

```{r}

Y <- st # Asignamos valores del data frame
t <- time(st) # asignamos variable tiempo de la serie
mes <- as.factor(month(df_stock$date))

mod.reg <- lm( Y ~ t + mes)

summary(mod.reg)

par(mfrow = c(2,2))

plot(mod.reg)

par(mfrow = c(1,1))

```


**Conclusión**

* El modelo de regresión general es muy significativo
* Según el modelo el mes 5 resulta significativo
* Se sospecha que no se cumple la normalidad en el gráfico de cuantiles, especialmente en los extremos.
* El $R^2$ = 0.817 es bueno (superior a 0.70)
* El RMSE = `r RMSE(mod.reg)` será utilizado como indicador para compararlo con otros modelos.

#### Predicción

Conforme la información disponible se tiene que el útlimo dato de la serie corresponde al 26 de junio de 2019 con valor de t=2019.444, el valor de la cotización del 24 de Mayo de 2019 fue: 178.97

Pronosticaremos el valor con base a la informaci?n disponible:

t = 2019.444 + 1/252

mes = 6

```{r}
t_actual <- max(time(st)) + 1/frecuencia_st

data_predict = data.frame(t= t_actual,
                          mes=5  )

data_predict$mes = as.factor(data_predict$mes)

# predict con intervalo de confianza del 95%
predict(mod.reg, newdata = data_predict, interval=c("confidence"), level = 0.95)



```

El valor de la acción para el 27 de junio 2019 de AAPL fue: 199.74


*** Graficando los valores observados y pronosticados**

```{r}

data_predict <- data.frame( t = time(st),
                            mes = month(df_stock$date))

data_predict$mes = as.factor(data_predict$mes)

# predict 
y_estimado <- predict(mod.reg, newdata = data_predict)


mmov <- ma(st, order = 10, centre = FALSE) # semanal

st_y_estimado <- ts( y_estimado, freq = frecuencia_st, start = c(2000,1,3) )

autoplot(cbind(st, st_y_estimado, mmov) , facets = FALSE) +
  labs(title = paste("Cotización de ", stock_name),
       x     = "tiempo",
       y     = "Precio")+
  theme(legend.position="bottom") +
  scale_colour_manual(labels = c("Real", "Reg. Lineal", "Media Movil 10"), 
                      values = c("steelblue2", "red", "green")) + 
  theme_minimal()


```

### Selección del mejor modelo de suavización exponencial



Carga de librerias
```{r}
library(fpp2)
library(tseries)
library(urca)
```

Definición de constantes
```{r}
THRESHOLD <- 0.7
LOW       <- 0
HIGH      <- 1
```


**Holt Winters : Aditivo**

```{r message=FALSE}
mod.HWA = HoltWinters(st, alpha = NULL, beta = NULL, gamma = NULL, seasonal = c("additive"))

sprintf('[ alpha = %f ][ beta = %f][ gamma = %f ] ',mod.HWA$alpha, mod.HWA$beta, mod.HWA$gamma)

if (mod.HWA$alpha == HIGH) {
  print('El nivel estimado del módelo depende del último valor')
} else if (mod.HWA$alpha == LOW){
  print('El nivel estimado del módelo solo depende de los valores historicos')
} else if (mod.HWA$alpha > THRESHOLD){
  print('El nivel estimado del módelo depende mayormente de los últimos valores, es cambiante')
} else {
  print('El nivel estimado del módelo depende mayormente de los valores historicos, se mantien constante∫')
}

if (mod.HWA$beta == HIGH) {
  print('La pendiente del del módelo depende del último valor')
} else if (mod.HWA$beta == LOW){
  print('La pendiente del módelo solo depende de los valores historicos')
} else if (mod.HWA$beta > THRESHOLD){
  print('La pendiente del módelo depende mayormente de los últimos valores, es cambiante')
} else {
  print('La pendiente del módelo depende mayormente de los valores historicos, se mantiene constante')
}

if (mod.HWA$gamma == HIGH) {
  print('La estacionalidad del módelo depende del último valor')
} else if (mod.HWA$gamma == LOW){
  print('La estacionalidad del módelo solo depende de los valores historicos')
} else if (mod.HWA$gamma > THRESHOLD){
  print('La estacionalidad del módelo depende mayormente de los últimos valores, es cambiante ')
} else {
  print('La estacionalidad del módelo depende mayormente de los valores historicos, se mantiene constante')
}

```

**Diagnóstico de los residuales**

```{r message=FALSE}
resid = residuals(mod.HWA)
autoplot(resid)
```


#### Conversion a serie de tiempo de retornos del activo

```{r}

R1 = timeSeries::returns(x = st, method = "discrete")[-1]
qplot(R1, geom = "histogram", fill = I("darkblue"), bins = 1+3.3*log10(length(R1)))

autoplot(ts(R1, freq = 252, start = c(2000,1)), ylab = "US$", main = "Retornos Discretos")

R2 = timeSeries::returns(st, method = "continuous")[-1]
autoplot(ts(R2, freq = 252, start = c(2000,1)), ylab = "US$", main = "Retornos Continuos")

qplot(R2, geom = "histogram", fill = I("darkblue"), bins = 1+3.3*log10(length(R2)))

```
Tanto R1 como R2 muestran aglomeracion de variaciones, por ejemplo, al inicio de la serie y en la parte central.

Trabajaremos con R2, los residuos al cuadrado.

Exploraremos algunas propiedades de la volatilidad.

```{r}
library(e1071)
kurtosis(R2)
```

La serie es claramente leptocurtica.

```{r}
shapiro.test(R2)
```

Los retornos al cuadrado muestran no normalidad.

```{r}
library(fractal)
stationarity(R2)

```

Las tres resultados anteriores parecen indicar que hay varianza condicional

Veamos que nos dice el posible modelamiento con ARIMA

```{r}
library(forecast)
(modelo.arima = auto.arima(R2))

```

Lo anterior sugiere usar ARMA(3,3)

```{r}
res.arima    = residuals(modelo.arima)

t.test(res.arima)
```

Residuales tienen media cero.

```{r}
shapiro.test(res.arima)
```

Residuales no siguen una distribucion normal.

```{r}
ggtsdisplay(res.arima)
```

Hay autocorrelacion de residuales. No ruido blanco.


```{r}
ggtsdisplay(res.arima^2)

```
ACF y PACF sugieren autocorrelacion de residuos al cuadrado con un desfase.

Probemos GARCH(1)

```{r}
library(fGarch)

(mod.arch1 = garchFit(~arma(3, 3) + garch(1, 0), 
                      data  = R2, 
                      trace = FALSE))
```

Los parametros de ARMA y de GARCH son altamente significativos.

Probemos con el modelo alternativo GARCH(1,1).
```{r}

(mod.garch11 = garchFit(~arma(3, 3) + garch(1, 1), 
                      data  = R2, 
                      trace = FALSE))
```

Tambien mustra parametros muy altamente significativos.

### Diagnostico GARCH

```{r}
res1  = mod.arch1@residuals/mod.arch1@sigma.t
res11 = mod.garch11@residuals/mod.garch11@sigma.t
```

##### Media cero de residuales

```{r}
t.test(res1)$p.value
t.test(res11)$p.value

```
ARCH(1) es OK

##### Normalidad de residuales

```{r}
shapiro.test(res1)$p.value
shapiro.test(res11)$p.value

```

No hay normalidad en residuales de los dos modelos

##### Resumen

```{r}
summary(mod.arch1)
summary(mod.garch11)
```

Ya que GARCH(1,0) tiene menor AIC, podemos escoger este modelo...

```{r}
ggtsdisplay(res1)
ggtsdisplay(res11)
```

... aunque GARCH(1,1) muestra mejor comportamiento de autocorrelacion de residuos

##### Pronóstico

Realizamos el pronostico con el mejor modelo: GARCH(1,0)

```{r}

autoplot(ts(mod.arch1@sigma.t))
autoplot(ts(mod.garch11@sigma.t))

predict(mod.arch1, n.ahead = 2)

```




### Neural Network


```{r}

library(keras)
library(astsa)


head(st)

# revisando retornos
st_return <- diff(log(st))

plot(st_return)

acf2(st_return)

auto.arima(st_return)


```

##


```{r}


# preparing data

# st_window: number of lags to construct matrix x to 
st_window <- 1

# nrow <- length(st_return) - st_window
nrow <- length(st) - st_window

data_model <- matrix( NA , nrow = nrow, ncol = st_window +1  )


for (i in 1:(st_window + 1)) {
  
  #data_model[1:nrow, i] = st_return[(st_window + 2 - i ):(nrow + st_window + 1 - i)]
  data_model[1:nrow, i] = st[(st_window + 2 - i ):(nrow + st_window + 1 - i)]
}

# Prepare variables: features and dependents
X <- array(data_model[ , 2:(st_window + 1)], dim=c(nrow, st_window , 1) ) 

y <- data_model[ , 1]

# Model  

model = keras_model_sequential() %>% 
  layer_lstm(units=128, input_shape=c(st_window, 1), activation="relu") %>%  
  layer_dense(units=64, activation = "relu") %>% 
  layer_dense(units=32) %>% 
  layer_dense(units=1, activation = "linear")


model %>% compile(loss = 'mse',
                  optimizer = 'adam',
                  metrics = list("mean_absolute_error")
                   )
 
model %>% summary()

history <- model %>% fit(X, y, epochs=50, 
                         batch_size=32, 
                         shuffle = FALSE, 
                         verbose=T,
                         validation_split = 0.20)

plot(history)

y_pred  <- model %>% predict(X)
 
scores  <- model %>% evaluate(X, y, verbose = T)

print(scores)


x_axes = seq(1:length(y_pred))
plot(x_axes, y, type="l", col="red", lwd=2)
lines(x_axes, y_pred, col="blue",lwd=2)
legend("topleft", legend=c("y-original", "y-predicted"),
        col=c("red", "blue"), lty=1,cex=0.8)



  
```

### Predicting

Pronosticamos valores para 5 observaciones (1 semana)

```{r}

# h_window: horizont to predict
h_window <- 5

data_predict <- numeric(h_window + 1)

data_predict[1] <- tail(y, 1)


for (i in 2:(h_window+1)) {
  
  X_predict <- array(data_predict[i-1], dim=c(1, st_window , 1) )
  
  X_value <- model %>% predict(X_predict)
  
  data_predict[i] <- X_value[1,1]
  
}

print(data_predict)

```








